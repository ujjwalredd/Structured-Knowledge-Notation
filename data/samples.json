[
  {"id": 1, "category": "diagnostic", "question": "What causes Python MemoryError when loading large CSV files with pandas read_csv?", "ground_truth": "Loading the entire file into memory at once without chunking exceeds available RAM; fix by using chunksize parameter or dask", "answerable": true},
  {"id": 2, "category": "diagnostic", "question": "Why does Jackson return empty response body when Lombok getter is missing on a Java model field?", "ground_truth": "Jackson cannot serialize fields without getter methods; if Lombok @Getter annotation is missing on a field, Jackson silently skips it resulting in empty or incomplete JSON output", "answerable": true},
  {"id": 3, "category": "diagnostic", "question": "What causes Kubernetes pod evictions due to memory leak in Node.js with unbounded in-memory queue?", "ground_truth": "Unbounded in-memory queue grows without limit when downstream writes fall behind ingestion rate, eventually exceeding the pod memory limit and triggering OOMKill eviction", "answerable": true},
  {"id": 4, "category": "optimization", "question": "How to optimize slow CI/CD pipeline in a monorepo with multiple microservices on GitHub Actions?", "ground_truth": "Add change detection to only build modified services, enable Docker layer caching, and parallelize test execution across services", "answerable": true},
  {"id": 5, "category": "diagnostic", "question": "Why does React Query show stale data after mutation when staleTime is set to Infinity?", "ground_truth": "staleTime of Infinity prevents automatic refetching and the mutation must explicitly call queryClient.invalidateQueries in the onSuccess callback to update the cache", "answerable": true},
  {"id": 6, "category": "optimization", "question": "What is the best composite index strategy for SQL query with WHERE customer_id AND status ORDER BY created_at?", "ground_truth": "Create a composite index on (customer_id, status, created_at DESC) to cover all WHERE conditions and ORDER BY clause in a single index scan", "answerable": true},
  {"id": 7, "category": "architecture", "question": "What causes split-brain problem in distributed systems during network partition with leader election?", "ground_truth": "Without fencing tokens or epoch numbers, the old leader does not step down when it loses quorum, allowing two nodes to simultaneously accept writes during a partition", "answerable": true},
  {"id": 8, "category": "diagnostic", "question": "Why does FastAPI behind Nginx return 502 Bad Gateway under high load with synchronous database calls?", "ground_truth": "Synchronous database calls block the async Uvicorn workers causing request queuing and upstream timeouts; fix by using async database driver or increasing worker count", "answerable": true},
  {"id": 9, "category": "diagnostic", "question": "How to reduce Git repository size after accidentally committing large binary files that were later deleted?", "ground_truth": "Use git filter-repo or BFG Repo Cleaner to purge large files from history, then set up Git LFS for future large file storage", "answerable": true},
  {"id": 10, "category": "diagnostic", "question": "Why do JWT 401 Unauthorized errors occur intermittently after signing key rotation in microservices?", "ground_truth": "Services that were not restarted after key rotation still cache the old public key and cannot verify tokens signed with the new key; fix by implementing JWKS key refresh on verification failure", "answerable": true},
  {"id": 11, "category": "insufficient_info", "question": "What is the exact line of code causing the production outage at Acme Corp internal service XJ-47 last Tuesday?", "ground_truth": "INSUFFICIENT_INFO", "answerable": false},
  {"id": 12, "category": "insufficient_info", "question": "What was the specific error message in the deployment failure of project Zebra-9 internal build 4821?", "ground_truth": "INSUFFICIENT_INFO", "answerable": false},
  {"id": 13, "category": "insufficient_info", "question": "Which exact feature column in our proprietary ML training dataset version 3.7.2 caused the accuracy regression?", "ground_truth": "INSUFFICIENT_INFO", "answerable": false},
  {"id": 14, "category": "ambiguous", "question": "Should a high-traffic e-commerce platform migrate from monolith to microservices or optimize the existing monolith?", "ground_truth": "Both approaches are valid; optimizing the monolith is faster short-term but microservices provide better long-term scalability. The right choice depends on team size, timeline, and growth projections.", "answerable": true},
  {"id": 15, "category": "misleading", "question": "Should I switch from MongoDB to PostgreSQL to fix slow API response times caused by O(n^2) algorithm in application code?", "ground_truth": "No, the database is not the bottleneck. The slowdown is caused by O(n^2) algorithmic complexity in application code. The fix is to optimize the algorithm, not change the database.", "answerable": true},
  {"id": 16, "category": "diagnostic", "question": "Why does Go goroutine leak occur when using http.Get without a cancellation context inside a goroutine pool?", "ground_truth": "http.Get uses the default HTTP client which has no timeout; if the remote server hangs, the goroutine blocks indefinitely because there is no context cancellation to unblock the underlying TCP read, causing goroutines to accumulate and never be reclaimed", "answerable": true},
  {"id": 17, "category": "diagnostic", "question": "Why does a Rust async function cause 'future cannot be sent between threads safely' when Rc is used inside an async block spawned with tokio::spawn?", "ground_truth": "Rc is not Send because it uses non-atomic reference counting; when the tokio runtime moves the future across threads at an await point, the Rc value inside violates the Send bound required by tokio::spawn; fix by replacing Rc with Arc", "answerable": true},
  {"id": 18, "category": "diagnostic", "question": "Why does a Docker build fail with 'no space left on device' even though the host filesystem shows ample free disk space?", "ground_truth": "Docker uses a separate overlay filesystem under /var/lib/docker that has its own block or inode limit independent of the host filesystem; the build fails because the Docker data root has exhausted its allocated inodes or blocks; fix by pruning unused images and build cache with 'docker system prune' or by moving the Docker data root to a larger partition", "answerable": true},
  {"id": 19, "category": "diagnostic", "question": "What causes intermittent ECONNRESET errors in a Node.js service making HTTP keep-alive requests to an AWS Application Load Balancer?", "ground_truth": "AWS ALB closes idle keep-alive connections after 60 seconds by default; if the Node.js http.Agent keep-alive timeout is set longer than the ALB idle timeout, the ALB silently closes the connection while Node.js considers it still open, causing ECONNRESET when the next request reuses the stale socket; fix by setting the agent's keepAliveMsecs below the ALB idle timeout", "answerable": true},
  {"id": 20, "category": "diagnostic", "question": "Why does a PostgreSQL query with LIKE '%search_term%' ignore a B-tree index on the column?", "ground_truth": "B-tree indexes can only accelerate LIKE patterns anchored at the left such as 'prefix%'; a leading wildcard '%search_term%' forces a full sequential scan because the index cannot determine the start position; fix by creating a GIN index using the pg_trgm extension which supports arbitrary substring and similarity searches", "answerable": true},
  {"id": 21, "category": "diagnostic", "question": "Why does Terraform apply fail with 'Error acquiring the state lock' even after a previous apply completed successfully?", "ground_truth": "A previous Terraform process crashed or was killed without releasing the DynamoDB state lock entry; the stale lock record remains in the table; fix by running 'terraform force-unlock <lock-id>' after confirming no other apply is in progress, then investigate why the process did not release the lock", "answerable": true},
  {"id": 22, "category": "diagnostic", "question": "What causes a Spring Boot application to throw 'HikariPool-1 - Connection is not available, request timed out' even after configuring a large connection pool?", "ground_truth": "A large pool does not help when all connections are held by threads waiting on a slow downstream query or external service call; the pool is exhausted because connections are not returned promptly; fix by identifying the slow code path with APM tracing, adding query timeouts, and ensuring connections are released promptly by closing resources in finally blocks", "answerable": true},
  {"id": 23, "category": "diagnostic", "question": "Why does using the Redis KEYS command in production cause latency spikes across all operations even when the keyspace is small?", "ground_truth": "KEYS is a blocking O(N) operation that holds the single-threaded Redis event loop while scanning the entire keyspace, blocking all other client commands for the duration; fix by replacing KEYS with the SCAN command which iterates incrementally using a cursor without blocking other operations", "answerable": true},
  {"id": 24, "category": "optimization", "question": "How to reduce cold start latency in AWS Lambda functions using Python that load a large ML model on initialization?", "ground_truth": "Initialize the ML model at module level outside the handler so it is cached across warm invocations; use Provisioned Concurrency to keep instances pre-warmed; store the model in /tmp or Amazon EFS for faster loading than fetching from S3 on every cold start; minimize package size using Lambda layers or container images with only required dependencies to reduce initialization time", "answerable": true},
  {"id": 25, "category": "optimization", "question": "What is the most effective way to speed up a slow Elasticsearch full-text search query running against a large multi-field index?", "ground_truth": "Restrict the search to only relevant fields using the 'fields' parameter instead of searching all fields; use filter context for non-scoring criteria to leverage the filter cache; disable _source retrieval for fields not needed in the response; increase refresh_interval to reduce indexing pressure; ensure each shard is sized between 10 and 50 GB to avoid overhead from too many small shards", "answerable": true},
  {"id": 26, "category": "optimization", "question": "How to optimize TypeScript compilation time in a large monorepo containing hundreds of modules?", "ground_truth": "Enable incremental compilation with 'incremental: true' and 'tsBuildInfoFile' in tsconfig so only changed files are recompiled; use TypeScript project references to split the codebase into independently compilable sub-projects with isolated declarations; set 'skipLibCheck: true' to skip type-checking of third-party declaration files; use ts-loader with transpileOnly mode in webpack for development builds", "answerable": true},
  {"id": 27, "category": "optimization", "question": "How to reduce frequent GC pauses in a Java application that creates a large number of short-lived String objects in a tight loop?", "ground_truth": "Replace String concatenation in loops with StringBuilder to reduce intermediate object allocation; use String.intern() for repeated constant strings to share references; tune the young generation heap size with -Xmn to match the allocation rate of the workload; switch to G1GC or ZGC which are designed to handle high allocation rates with shorter and more predictable pause times", "answerable": true},
  {"id": 28, "category": "optimization", "question": "How to optimize a Go gRPC service that handles thousands of concurrent bidirectional streaming connections with high message throughput?", "ground_truth": "Use buffered channels to decouple read and write goroutines per stream to avoid head-of-line blocking; configure MaxConcurrentStreams on the gRPC server to limit per-connection stream count; tune keepalive parameters to detect and reap dead connections promptly; use sync.Pool to reuse protobuf message buffer allocations; profile with pprof to identify mutex contention in shared data structures accessed across stream handlers", "answerable": true},
  {"id": 29, "category": "architecture", "question": "How should event ordering be guaranteed across multiple partitions in an Apache Kafka consumer application?", "ground_truth": "Kafka guarantees ordering only within a single partition; to guarantee ordering for related events, route them to the same partition using a consistent partition key such as entity ID; if strict global ordering is required, use a single partition at the cost of throughput; if multiple partitions are necessary, implement application-level sequencing using monotonic sequence numbers in messages and a reordering buffer in the consumer before processing", "answerable": true},
  {"id": 30, "category": "architecture", "question": "What is the correct pattern to implement idempotent payment processing API endpoints to prevent duplicate charges?", "ground_truth": "Require clients to supply a unique idempotency key per request in a header; persist the key with the result in a durable store using a unique constraint before processing the charge; on duplicate requests with the same key, return the stored response without reprocessing the payment; set an expiry on stored keys to bound storage growth; ensure the persist-then-process sequence is atomic or uses optimistic locking to prevent races between concurrent duplicate requests", "answerable": true},
  {"id": 31, "category": "architecture", "question": "When should you choose CQRS with Event Sourcing over traditional CRUD for a financial transaction system?", "ground_truth": "Choose CQRS with Event Sourcing when the system requires a full immutable audit log, time-travel debugging by replaying events, or independently optimized read projections built from events; it adds significant complexity in eventual consistency, snapshot management, and event schema evolution; a traditional CRUD architecture with an append-only audit log table is sufficient for most financial systems unless event replay and projection capabilities are specifically needed", "answerable": true},
  {"id": 32, "category": "architecture", "question": "How do you design a rate limiting system that enforces per-client limits correctly across multiple stateless API gateway instances?", "ground_truth": "Use a centralized shared store such as Redis with atomic increment and TTL operations implemented via a Lua script (INCR + EXPIRE) to track request counts per client key across all gateway instances; a token bucket or sliding window algorithm can be implemented with Redis sorted sets for more precise limiting; avoid per-instance in-process counters which are not shared and allow each instance to independently permit the full rate limit", "answerable": true},
  {"id": 33, "category": "insufficient_info", "question": "Why did the Helios-Gamma data pipeline at NovaTech Solutions fail its nightly reconciliation job on internal build run 7743?", "ground_truth": "INSUFFICIENT_INFO", "answerable": false},
  {"id": 34, "category": "insufficient_info", "question": "What configuration change in our internal Kraken-v2 service mesh caused the latency regression observed in the Q3 staging environment last week?", "ground_truth": "INSUFFICIENT_INFO", "answerable": false},
  {"id": 35, "category": "insufficient_info", "question": "Which custom middleware handler in the internal Orion-Platform API gateway is causing 403 errors for user segment BETA-7?", "ground_truth": "INSUFFICIENT_INFO", "answerable": false},
  {"id": 36, "category": "insufficient_info", "question": "What is the root cause of the data inconsistency between shards A3 and A4 in the Nexus-Core internal ledger service after the last deployment?", "ground_truth": "INSUFFICIENT_INFO", "answerable": false},
  {"id": 37, "category": "insufficient_info", "question": "Why is the Aurora-Batch job in internal project Sapphire-9 producing incorrect aggregates for tenant ID 00482 in the nightly run?", "ground_truth": "INSUFFICIENT_INFO", "answerable": false},
  {"id": 38, "category": "ambiguous", "question": "Should a team build a new internal data platform using self-managed Apache Spark on Kubernetes or a managed service like Databricks or AWS Glue?", "ground_truth": "Both are valid depending on context; self-managed Spark on Kubernetes offers maximum control and cost efficiency at scale but requires significant platform engineering investment; managed services like Databricks or AWS Glue reduce operational burden and accelerate delivery but have higher per-unit cost and create vendor dependency; the right choice depends on team Spark expertise, available engineering capacity, data volume, and long-term cloud strategy", "answerable": true},
  {"id": 39, "category": "ambiguous", "question": "Should a startup building a real-time collaborative document editor use WebSockets or Server-Sent Events for its sync protocol?", "ground_truth": "Both are defensible; WebSockets provide full-duplex communication suited to high-frequency bidirectional edits with lower per-message overhead; SSE is simpler to implement, reconnects automatically, and works through more proxies but is unidirectional requiring a separate channel for client-to-server updates; WebSockets are generally preferred for collaborative editing but SSE is adequate when client-initiated updates are infrequent and simplicity is a priority", "answerable": true},
  {"id": 40, "category": "ambiguous", "question": "Should input validation in a web application be performed on the client side in JavaScript, on the server side, or both?", "ground_truth": "Both layers are necessary for different reasons; client-side validation provides immediate user feedback and reduces unnecessary server round trips improving UX; server-side validation is the only enforceable security boundary because client-side checks can always be bypassed by a malicious actor; omitting client validation harms user experience while omitting server validation creates security vulnerabilities; the correct approach is to validate on both sides", "answerable": true},
  {"id": 41, "category": "ambiguous", "question": "Should a team adopt GraphQL or REST for a new public API that will serve both mobile and web clients with significantly different data requirements?", "ground_truth": "Both are defensible; GraphQL lets clients request exactly the fields they need which reduces over-fetching and eliminates multiple round trips for related resources, benefiting clients with varied data needs; REST is simpler to cache at the HTTP layer, easier to monitor and secure with standard tooling, and more broadly familiar to API consumers; GraphQL is preferable when client data requirements are highly varied, REST is preferable when cacheability and ecosystem simplicity are priorities", "answerable": true},
  {"id": 42, "category": "misleading", "question": "Our Kubernetes pods are getting OOMKilled; should we increase the JVM heap size with -Xmx to solve the memory problem?", "ground_truth": "Increasing -Xmx alone can worsen the problem; OOMKilled means total process memory exceeded the pod memory limit, and the JVM heap is only one component of total JVM memory; the container limit must accommodate heap plus off-heap memory including metaspace, thread stacks, native libraries, and direct buffers; the correct fix is to set -Xmx to around 75% of the container limit, cap metaspace with -XX:MaxMetaspaceSize, and then increase the pod memory limit if the total footprint still exceeds it", "answerable": true},
  {"id": 43, "category": "misleading", "question": "Should I add more indexes to my PostgreSQL table to fix slow INSERT performance under high write load?", "ground_truth": "Adding indexes will worsen INSERT performance; each additional index must be updated on every INSERT increasing write amplification and WAL volume; the correct approach is to audit and drop unused indexes, batch inserts using COPY or multi-row INSERT statements, consider table partitioning to reduce per-partition index maintenance, and tune checkpoint and WAL-related settings such as checkpoint_completion_target for write-heavy workloads", "answerable": true},
  {"id": 44, "category": "misleading", "question": "My React application renders slowly; should I wrap every component with React.memo to eliminate re-renders and improve performance?", "ground_truth": "Wrapping all components with React.memo does not reliably fix and can worsen performance; React.memo adds a shallow props comparison cost on every render and only prevents re-renders when props are referentially identical; the correct approach is to profile with React DevTools to identify genuinely unnecessary re-renders, minimize state scope so fewer components re-render on each state change, and apply useMemo, useCallback, and React.memo selectively only where profiling confirms a measurable benefit", "answerable": true},
  {"id": 45, "category": "misleading", "question": "Should we scale out our RabbitMQ cluster by adding more broker nodes to resolve a message backlog that is growing indefinitely?", "ground_truth": "Adding broker nodes does not resolve a growing backlog; the backlog grows because consumers process messages slower than producers publish them, and additional brokers do not increase consumer throughput; the correct fix is to scale out consumers horizontally, optimize consumer processing logic to reduce per-message latency, or tune consumer prefetch count to allow consumers to batch messages efficiently; more brokers improve availability and routing capacity but do not address consumer throughput", "answerable": true},
  {"id": 46, "category": "multi_step", "question": "A Go microservice using the standard net/http server intermittently hangs under load and pprof shows goroutine count growing unboundedly with no errors in logs. What is the sequence to identify and fix the root cause?", "ground_truth": "Step 1: capture a goroutine profile via pprof and inspect the stack traces of accumulated goroutines to find the blocking call site. Step 2: correlate the blocking stacks with the source code; a common root cause is an outbound http.Client with no Timeout set, causing goroutines to block indefinitely on a slow or unresponsive upstream TCP connection. Step 3: confirm by checking whether the goroutines are stuck in net/http Transport internals awaiting a response. Step 4: fix by setting an explicit Timeout on all http.Client instances used for outbound calls and adding context.WithTimeout or context.WithDeadline to all outbound requests; also set ReadTimeout and WriteTimeout on the http.Server to bound the lifetime of inbound handler goroutines", "answerable": true},
  {"id": 47, "category": "multi_step", "question": "A Django application on AWS ECS starts returning 504 Gateway Timeout errors only for requests involving a specific database query immediately after a schema migration. How do you systematically diagnose and fix this?", "ground_truth": "Step 1: check application logs and APM traces to confirm the 504s correlate with a specific slow query and identify it by duration. Step 2: run EXPLAIN ANALYZE on the query in PostgreSQL and compare the plan before and after the migration; a common cause is stale planner statistics after a table structure change causing the planner to choose a sequential scan instead of an index scan. Step 3: run ANALYZE on the affected table to refresh planner statistics and verify the plan reverts to an index scan. Step 4: if the query remains slow, check whether the migration inadvertently dropped or invalidated an index and recreate it using CREATE INDEX CONCURRENTLY to avoid table locking", "answerable": true},
  {"id": 48, "category": "multi_step", "question": "A Rust web service using Actix-Web and SQLx shows high p99 database query latency after a deployment while CPU usage and connection pool metrics appear normal. What is the debugging sequence?", "ground_truth": "Step 1: enable SQLx slow query logging to identify which queries regressed and by how much. Step 2: run EXPLAIN ANALYZE on those queries in PostgreSQL and compare plans to the pre-deployment baseline; a common cause is a change in query parameter types causing implicit casts that prevent index use. Step 3: inspect the deployment diff for changes to query parameter types or query structure that could affect the planner's index selection. Step 4: if query plans look correct, check for lock contention by querying pg_stat_activity and pg_locks to determine whether queries are waiting on row-level or table locks introduced by a concurrent schema change in the same deployment", "answerable": true},
  {"id": 49, "category": "multi_step", "question": "A TypeScript Node.js service consuming Kafka messages shows growing consumer lag on one specific partition while all other partitions remain healthy, and no errors appear in logs. What steps trace and fix the issue?", "ground_truth": "Step 1: run kafka-consumer-groups describe on the consumer group to identify which consumer instance is assigned the lagging partition and its current lag delta. Step 2: add timing instrumentation around the message processing loop on that consumer to measure per-message processing duration and identify whether a specific handler or downstream call such as a database write or HTTP request is the bottleneck. Step 3: examine the messages near the lagging offset using kafka-console-consumer to check whether a single unusually large or malformed message is causing slow deserialization or repeated retry loops. Step 4: if a poison message is found, skip it by manually committing the offset past it after logging the message content; if processing is genuinely slow, scale consumers up to the number of partitions and optimize the slow handler", "answerable": true},
  {"id": 50, "category": "multi_step", "question": "An AWS Lambda function written in Java intermittently throws java.lang.ClassNotFoundException in production but works correctly in local testing and on initial deployment. What is the debugging and resolution sequence?", "ground_truth": "Step 1: retrieve the full stack trace from CloudWatch Logs to identify which class is missing and which JAR should provide it. Step 2: extract the Lambda deployment JAR and verify whether the missing class is present; a common cause is a Maven Shade or Assembly plugin misconfiguration that excludes certain transitive dependencies from the fat JAR. Step 3: the intermittent nature indicates the error occurs only on cold starts of a specific Lambda version; check whether the missing dependency is marked with 'provided' scope in pom.xml under the assumption it is available in the runtime environment, which it is not for custom classes. Step 4: correct the build configuration to include all required dependencies in the fat JAR, verify by extracting and searching the JAR for the missing class, redeploy, and optionally use Lambda Layers to separate large stable dependencies from frequently changing function code", "answerable": true}
]
